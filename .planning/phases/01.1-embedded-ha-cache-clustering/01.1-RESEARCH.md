# Phase 1.1: Embedded HA Cache Clustering - Research

**Researched:** 2026-01-21
**Domain:** Olric distributed cache clustering with hashicorp/memberlist
**Confidence:** HIGH

## Summary

This research investigates how to enable cc-relay instances to form HA cache clusters using embedded Olric nodes. The investigation covers Olric v0.7.2 (the current version in go.mod), memberlist-based node discovery, data replication configuration, quorum settings, and graceful shutdown semantics.

**Key findings:**
- Olric provides comprehensive clustering out-of-the-box via hashicorp/memberlist
- Configuration is straightforward: `config.New(env)` with env = "local", "lan", or "wan"
- ReplicaCount, ReadQuorum, WriteQuorum are top-level config fields (not nested)
- Graceful shutdown involves Leave() followed by Shutdown() - Olric handles this internally
- Multi-node testing patterns are well-established in Olric's own test suite

**Primary recommendation:** Extend OlricConfig with `Environment`, `ReplicaCount`, `ReadQuorum`, `WriteQuorum`, and `MemberCountQuorum` fields. Use `config.New(env)` to get sensible defaults per deployment type.

## Standard Stack

The established libraries/tools for this domain:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| github.com/olric-data/olric | v0.7.2 | Distributed cache with embedded clustering | Already in use, provides all HA features |
| github.com/hashicorp/memberlist | (transitive) | Gossip-based cluster membership | Used internally by Olric, battle-tested |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| testify | v1.x | Testing assertions | Already in use for tests |
| zerolog | v1.x | Structured logging | Already in use |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Olric embedded | External Redis cluster | More ops complexity, separate service to manage |
| memberlist static peers | Consul/K8s service discovery | Added dependency, more complex config |

**No new dependencies required** - all clustering capabilities come from existing Olric dependency.

## Architecture Patterns

### Recommended OlricConfig Structure Extension

```go
// OlricConfig configures the Olric distributed cache.
type OlricConfig struct {
    // Existing fields (keep for backwards compatibility)
    DMapName  string   `yaml:"dmap_name"`
    BindAddr  string   `yaml:"bind_addr"`
    Addresses []string `yaml:"addresses"`
    Peers     []string `yaml:"peers"`
    Embedded  bool     `yaml:"embedded"`

    // NEW: HA clustering configuration
    // Environment selects memberlist configuration preset.
    // Valid values: "local" (development/loopback), "lan" (same datacenter), "wan" (cross-datacenter)
    // Default: "local" if not specified
    Environment string `yaml:"environment"`

    // ReplicaCount is the number of replicas for each key.
    // 1 = no replication (single copy), 2+ = replicated across nodes.
    // Default: 1 (Olric default)
    ReplicaCount int `yaml:"replica_count"`

    // ReadQuorum is minimum successful reads required for a read response.
    // Must be <= ReplicaCount. 1 = read from any replica.
    // Default: 1
    ReadQuorum int `yaml:"read_quorum"`

    // WriteQuorum is minimum successful writes required for a write response.
    // Must be <= ReplicaCount. 1 = fire-and-forget to replicas.
    // Default: 1
    WriteQuorum int `yaml:"write_quorum"`

    // MemberCountQuorum is minimum cluster members required to operate.
    // Prevents split-brain scenarios by refusing operations with too few nodes.
    // Default: 1 (single node okay)
    MemberCountQuorum int32 `yaml:"member_count_quorum"`

    // LeaveTimeout is maximum time to wait for leave message broadcast.
    // Default: 5s (Olric default)
    LeaveTimeout time.Duration `yaml:"leave_timeout"`
}
```

### Recommended Project Structure Addition

```
internal/cache/
├── cache.go           # Interface (existing)
├── config.go          # Config types (extend OlricConfig)
├── factory.go         # Factory function (existing)
├── noop.go            # Noop adapter (existing)
├── olric.go           # Olric adapter (modify for HA)
├── olric_test.go      # Unit tests (extend)
├── olric_cluster_test.go  # NEW: Multi-node integration tests
├── ristretto.go       # Ristretto adapter (existing)
└── testutil.go        # NEW: Test helpers for cluster setup
```

### Pattern 1: Embedded Cluster Node Configuration

**What:** Configure an embedded Olric node with HA settings
**When to use:** Production deployments where cc-relay instances should share cache state
**Example:**
```go
// Source: Olric v0.7.2 documentation and olric_test.go
func buildOlricConfig(cfg *OlricConfig) (*olricconfig.Config, error) {
    // Select environment preset (sets memberlist timeouts, gossip intervals)
    env := cfg.Environment
    if env == "" {
        env = "local"
    }

    c := olricconfig.New(env) // Creates config with sane defaults for environment

    // Parse bind address
    bindAddr, bindPort := parseBindAddr(cfg.BindAddr)
    c.BindAddr = bindAddr
    if bindPort > 0 {
        c.BindPort = bindPort
    }

    // Clustering configuration
    c.ReplicaCount = cfg.ReplicaCount
    if c.ReplicaCount < 1 {
        c.ReplicaCount = 1 // Olric default
    }

    c.ReadQuorum = cfg.ReadQuorum
    if c.ReadQuorum < 1 {
        c.ReadQuorum = 1
    }

    c.WriteQuorum = cfg.WriteQuorum
    if c.WriteQuorum < 1 {
        c.WriteQuorum = 1
    }

    c.MemberCountQuorum = cfg.MemberCountQuorum
    if c.MemberCountQuorum < 1 {
        c.MemberCountQuorum = 1
    }

    // Peer discovery
    if len(cfg.Peers) > 0 {
        c.Peers = cfg.Peers
    }

    // Leave timeout for graceful shutdown
    if cfg.LeaveTimeout > 0 {
        c.LeaveTimeout = cfg.LeaveTimeout
    }

    // Suppress verbose logging
    c.LogOutput = io.Discard
    c.Logger = log.New(io.Discard, "", 0)

    return c, nil
}
```

### Pattern 2: Node Startup and Cluster Join

**What:** Start embedded node and join existing cluster
**When to use:** When starting a new cc-relay instance
**Example:**
```go
// Source: Olric olric_test.go - testOlricCluster pattern
func newEmbeddedOlricCache(ctx context.Context, cfg *OlricConfig, dmapName string, lg *zerolog.Logger) (*olricCache, error) {
    c, err := buildOlricConfig(cfg)
    if err != nil {
        return nil, err
    }

    // Ready signal channel - MUST be set before olric.New()
    ready := make(chan struct{})
    c.Started = func() {
        close(ready)
    }

    // Create Olric instance
    db, err := olric.New(c)
    if err != nil {
        return nil, err
    }

    // Start in background goroutine
    startErr := make(chan error, 1)
    go func() {
        if err := db.Start(); err != nil {
            startErr <- err
        }
    }()

    // Wait for ready or timeout
    startupCtx, cancel := context.WithTimeout(ctx, 10*time.Second)
    defer cancel()

    select {
    case <-ready:
        lg.Debug().Msg("olric: embedded node ready")
    case err := <-startErr:
        return nil, fmt.Errorf("olric startup failed: %w", err)
    case <-startupCtx.Done():
        // Node still starting but should be usable
        time.Sleep(100 * time.Millisecond)
    }

    // Get embedded client
    client := db.NewEmbeddedClient()
    dm, err := client.NewDMap(dmapName)
    if err != nil {
        db.Shutdown(context.Background())
        return nil, err
    }

    return &olricCache{
        client: client,
        dmap:   dm,
        db:     db,
        name:   dmapName,
        log:    lg,
    }, nil
}
```

### Pattern 3: Graceful Shutdown with Cluster Leave

**What:** Properly leave cluster before shutting down
**When to use:** During cc-relay shutdown/restart
**Example:**
```go
// Source: Olric internal/discovery/discovery.go Shutdown()
// Olric's Shutdown() already handles Leave() internally:
// 1. Broadcasts leave message with LeaveTimeout
// 2. Stops background listeners
// 3. Cleans up resources

func (o *olricCache) Close() error {
    if o.closed.Load() {
        return nil
    }

    o.mu.Lock()
    defer o.mu.Unlock()

    if o.closed.Load() {
        return nil
    }
    o.closed.Store(true)

    ctx := context.Background()

    // Close DMap handle first
    if o.dmap != nil {
        if err := o.dmap.Close(ctx); err != nil {
            o.log.Debug().Err(err).Msg("olric: dmap close error")
        }
    }

    if o.db != nil {
        // Shutdown handles Leave() + cleanup internally
        // Uses LeaveTimeout from config (default 5s)
        if err := o.db.Shutdown(ctx); err != nil {
            o.log.Error().Err(err).Msg("olric: shutdown error")
            return err
        }
        o.log.Info().Msg("olric: embedded node gracefully shutdown")
    } else if o.client != nil {
        // Client mode - just close connection
        if err := o.client.Close(ctx); err != nil {
            return err
        }
    }

    return nil
}
```

### Anti-Patterns to Avoid

- **Hardcoding memberlist config:** Don't manually configure memberlist timeouts. Use `config.New(env)` to get tested defaults for your environment.

- **Ignoring LeaveTimeout:** Don't set LeaveTimeout to 0 or very short durations. This prevents proper leave message propagation, causing surviving nodes to think the departed node crashed.

- **WriteQuorum > ReplicaCount:** Invalid configuration. WriteQuorum must be <= ReplicaCount.

- **MemberCountQuorum too high for expected cluster size:** If you set MemberCountQuorum=3 but only run 2 nodes, all operations will fail.

- **Missing Started callback:** Always set `c.Started` callback before calling `olric.New()`. This is the reliable way to know when the node is ready.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Node discovery | Custom peer finding | memberlist via Olric config.Peers | memberlist handles gossip, failure detection, membership |
| Split-brain prevention | Custom voting | MemberCountQuorum | Olric implements this correctly |
| Data replication | Manual copy to peers | ReplicaCount + WriteQuorum | Olric handles partition distribution |
| Graceful leave | Custom leave protocol | Olric Shutdown() | Internally calls memberlist.Leave() with proper timeout |
| Consistent hashing | Custom ring | Olric's partition system | PartitionCount + consistent hash built-in |

**Key insight:** Olric provides a complete clustering solution. Extending OlricConfig to expose these settings is straightforward - don't reinvent the clustering layer.

## Common Pitfalls

### Pitfall 1: Port Conflicts in Tests

**What goes wrong:** Multiple test nodes try to bind to same port, causing test failures
**Why it happens:** Tests run in parallel without unique port allocation
**How to avoid:** Use atomic counter for port allocation (already implemented in olric_test.go)
```go
var portCounter atomic.Int32
func init() { portCounter.Store(13320) }
func getNextPort() int { return int(portCounter.Add(1)) }
```
**Warning signs:** "address already in use" errors in test output

### Pitfall 2: Startup Race Condition

**What goes wrong:** Operations fail with "not ready yet" errors
**Why it happens:** Attempting cache operations before node fully joins cluster
**How to avoid:** Wait for `Started` callback before returning from constructor
```go
c.Started = func() { close(ready) }
<-ready // Wait for this signal
```
**Warning signs:** `ErrNotReady` errors on first operations

### Pitfall 3: Incomplete Shutdown Leading to Ghost Nodes

**What goes wrong:** Surviving nodes still think departed node is alive
**Why it happens:** Node crashed without broadcasting leave message
**How to avoid:**
- Use proper shutdown sequence (signal handlers)
- Set reasonable LeaveTimeout (default 5s is good)
- Graceful shutdown path should always be used
**Warning signs:** Stats show more ClusterMembers than actually running

### Pitfall 4: Quorum Misconfiguration

**What goes wrong:** Operations hang or fail when they shouldn't
**Why it happens:** WriteQuorum > available replicas, or MemberCountQuorum > cluster size
**How to avoid:** Validate configuration at startup:
```go
if cfg.WriteQuorum > cfg.ReplicaCount {
    return errors.New("write_quorum cannot exceed replica_count")
}
```
**Warning signs:** Timeout errors, "operation timeout" from Olric

### Pitfall 5: Environment Mismatch

**What goes wrong:** Nodes can't discover each other, or timeouts in LAN environment
**Why it happens:** Using "local" environment in production, or "wan" in same datacenter
**How to avoid:**
- Use "local" only for development/testing on loopback
- Use "lan" for same datacenter deployments
- Use "wan" for cross-datacenter (rare for cc-relay)
**Warning signs:** Slow cluster convergence, missed heartbeats

## Code Examples

### Complete HA Configuration Example

```yaml
# Source: Derived from Olric olric-server-local.yaml
cache:
  mode: ha
  olric:
    embedded: true
    bind_addr: "0.0.0.0:3320"
    dmap_name: "cc-relay-cache"

    # Peer discovery - list at least one existing node
    peers:
      - "cc-relay-1:3322"  # Note: memberlist port (3322), not Olric port (3320)
      - "cc-relay-2:3322"

    # HA settings
    environment: lan        # "local", "lan", or "wan"
    replica_count: 2        # Store 2 copies of each key
    read_quorum: 1          # Read from any replica
    write_quorum: 1         # Fire-and-forget to replicas (async)
    member_count_quorum: 2  # Require 2 nodes to operate

    # Graceful shutdown
    leave_timeout: 5s
```

### Multi-Node Test Setup Pattern

```go
// Source: Olric olric_test.go testOlricCluster
type testCacheCluster struct {
    mtx     sync.Mutex
    members []*olricCache
    t       *testing.T
}

func newTestCacheCluster(t *testing.T) *testCacheCluster {
    cl := &testCacheCluster{members: make([]*olricCache, 0), t: t}
    t.Cleanup(func() {
        cl.mtx.Lock()
        defer cl.mtx.Unlock()
        for _, m := range cl.members {
            m.Close()
        }
    })
    return cl
}

func (cl *testCacheCluster) addMember() *olricCache {
    cl.mtx.Lock()
    defer cl.mtx.Unlock()

    port := getNextPort()
    cfg := &OlricConfig{
        DMapName:     "test-dmap",
        Embedded:     true,
        BindAddr:     fmt.Sprintf("127.0.0.1:%d", port),
        Environment:  "local",
        ReplicaCount: 2,
    }

    // Add existing members as peers
    for _, m := range cl.members {
        // Use memberlist port (BindPort from memberlist config)
        cfg.Peers = append(cfg.Peers, m.memberlistAddr())
    }

    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    cache, err := newOlricCache(ctx, cfg)
    require.NoError(cl.t, err)

    cl.members = append(cl.members, cache)
    return cache
}
```

### Verifying Cluster Formation

```go
// Source: Olric embedded_client.go Stats method
func verifyClusterSize(t *testing.T, cache *olricCache, expectedMembers int) {
    // Stats requires member address
    e := cache.db.NewEmbeddedClient()
    thisNode := cache.db.rt.This().String() // Get this node's address

    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    st, err := e.Stats(ctx, thisNode)
    require.NoError(t, err)
    require.Len(t, st.ClusterMembers, expectedMembers,
        "expected %d cluster members, got %d", expectedMembers, len(st.ClusterMembers))
}
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| buraksezer/olric | olric-data/olric | v0.6.0 (2024) | Module rename - update import paths |
| Manual memberlist config | config.New(env) presets | Always | Simpler, tested defaults |
| Sequential read-repair | Parallel read-repair | v0.7.0 | Lower latency for ReadQuorum > 1 |
| Sequential replica reads | Parallel replica reads | v0.7.0 | Faster reads when ReadQuorum > 1 |

**Deprecated/outdated:**
- Import path `github.com/buraksezer/olric` - redirects to `github.com/olric-data/olric` but should be updated

## Open Questions

Things that couldn't be fully resolved:

1. **Memberlist port exposure**
   - What we know: Olric uses port 3320 for data, memberlist uses 3322 by default
   - What's unclear: Do we need to expose memberlist port configuration separately?
   - Recommendation: For now, let Olric handle memberlist port (it uses BindPort+2 by default). Add explicit `memberlist_port` config only if needed.

2. **Kubernetes service discovery**
   - What we know: Olric supports K8s discovery via plugin
   - What's unclear: Is this needed for cc-relay's use case?
   - Recommendation: Start with static peers in Peers list. Add K8s discovery as Phase 1.2 if needed.

3. **Partition count tuning**
   - What we know: Default is 271 partitions, affects data distribution
   - What's unclear: Optimal value for cc-relay's expected data size and cluster size
   - Recommendation: Keep default (271). Only tune if benchmarks show issues.

4. **Read-repair implications**
   - What we know: ReadRepair=true helps reduce entropy after failures
   - What's unclear: Performance impact for cache use case (vs. primary datastore)
   - Recommendation: Leave disabled (default) since cache data is not critical. Can enable if consistency issues arise.

## Sources

### Primary (HIGH confidence)
- Olric v0.7.2 source code - `go doc` output for Config, NewMemberlistConfig
- Olric v0.7.2 test files - olric_test.go, integration_test.go, testutil.go
- Olric v0.7.2 sample config - cmd/olric-server/olric-server-local.yaml
- cc-relay existing code - internal/cache/olric.go, config.go, olric_test.go

### Secondary (MEDIUM confidence)
- [GitHub olric-data/olric README](https://github.com/olric-data/olric) - Architecture overview
- [Olric Docker README](https://github.com/olric-data/olric/blob/master/docker/README.md) - Multi-node testing patterns

### Tertiary (LOW confidence)
- WebSearch results - General clustering patterns (verified against source)

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Using existing Olric dependency, no new libraries
- Architecture: HIGH - Based on Olric's own test patterns and config structure
- Configuration: HIGH - Directly from Olric source code and go doc
- Pitfalls: HIGH - Identified from Olric test suite and source code comments
- Testing patterns: HIGH - Copied from Olric's own integration tests

**Research date:** 2026-01-21
**Valid until:** ~90 days (Olric is stable, v0.7.2 released 2024)

## Appendix: Key Olric Config Fields Reference

```go
// From github.com/olric-data/olric/config/config.go
type Config struct {
    // Networking
    BindAddr string
    BindPort int      // Default: 3320

    // Clustering
    Peers               []string      // Static peer list for discovery
    PartitionCount      uint64        // Default: 271
    ReplicaCount        int           // Default: 1 (no replication)
    ReadQuorum          int           // Default: 1
    WriteQuorum         int           // Default: 1
    MemberCountQuorum   int32         // Default: 1

    // Replication mode
    ReplicationMode     int           // 0=sync, 1=async
    ReadRepair          bool          // Enable read-repair

    // Timeouts
    BootstrapTimeout    time.Duration // Default: computed
    LeaveTimeout        time.Duration // Default: 5s

    // Callbacks
    Started             func()        // Called when node is ready

    // Memberlist
    MemberlistConfig    *memberlist.Config // Use NewMemberlistConfig(env) to create

    // Logging
    LogOutput           io.Writer
    Logger              *log.Logger
}
```

## Appendix: Environment Presets Comparison

| Setting | local | lan | wan |
|---------|-------|-----|-----|
| TCPTimeout | 10s | 10s | 30s |
| IndirectChecks | 1 | 3 | 3 |
| RetransmitMult | 2 | 4 | 4 |
| SuspicionMult | 3 | 4 | 6 |
| PushPullInterval | 15s | 30s | 60s |
| ProbeInterval | 1s | 1s | 2s |
| ProbeTimeout | 200ms | 500ms | 3s |
| GossipInterval | 100ms | 200ms | 500ms |

Use `local` for development, `lan` for production within same datacenter.
