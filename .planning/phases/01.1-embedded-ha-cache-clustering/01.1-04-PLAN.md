---
phase: 01.1-embedded-ha-cache-clustering
plan: 04
type: execute
wave: 3
depends_on: ["01.1-02", "01.1-03"]
files_modified:
  - internal/cache/olric_cluster_test.go
  - internal/cache/testutil.go
autonomous: true

must_haves:
  truths:
    - "Multiple embedded Olric nodes can form a cluster"
    - "Cluster nodes discover each other via Peers list"
    - "Data written to one node is readable from another node"
    - "Node leaving cluster does not break remaining nodes"
    - "New nodes can join existing cluster dynamically"
  artifacts:
    - path: "internal/cache/olric_cluster_test.go"
      provides: "Multi-node cluster integration tests"
      contains: "func TestOlricCluster"
    - path: "internal/cache/testutil.go"
      provides: "Test cluster helper utilities"
      contains: "type testCacheCluster struct"
  key_links:
    - from: "internal/cache/olric_cluster_test.go"
      to: "internal/cache/testutil.go"
      via: "testCacheCluster helper"
      pattern: "newTestCacheCluster"
    - from: "testCacheCluster"
      to: "OlricConfig.Peers"
      via: "Dynamic peer list construction"
      pattern: "cfg\\.Peers"
---

<objective>
Create integration tests for multi-node Olric clustering

Purpose: Verify that multiple cc-relay instances can form HA cache clusters, share data, and handle node joins/leaves gracefully. These tests validate the core HA requirements (CACHE-HA-01, CACHE-HA-02, CACHE-HA-03).

Output: Comprehensive multi-node cluster tests with reusable test utilities
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01.1-embedded-ha-cache-clustering/01.1-RESEARCH.md
@.planning/phases/01.1-embedded-ha-cache-clustering/01.1-01-SUMMARY.md
@.planning/phases/01.1-embedded-ha-cache-clustering/01.1-02-SUMMARY.md
@.planning/phases/01.1-embedded-ha-cache-clustering/01.1-03-SUMMARY.md

# Existing test patterns
@internal/cache/olric_test.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create testutil.go with cluster test helpers</name>
  <files>internal/cache/testutil.go</files>
  <action>
Create a new file with reusable test utilities for cluster testing:

```go
package cache

import (
    "context"
    "fmt"
    "sync"
    "testing"
    "time"
)

// testCacheCluster manages a group of embedded Olric nodes for testing.
// It handles node creation, peer discovery, and cleanup.
type testCacheCluster struct {
    mtx     sync.Mutex
    members []*olricCache
    t       *testing.T
    dmapName string
}

// newTestCacheCluster creates a new test cluster.
// All nodes are cleaned up when the test completes.
func newTestCacheCluster(t *testing.T) *testCacheCluster {
    t.Helper()

    cl := &testCacheCluster{
        members:  make([]*olricCache, 0),
        t:        t,
        dmapName: fmt.Sprintf("test-cluster-%d", getNextPort()),
    }

    t.Cleanup(func() {
        cl.shutdown()
    })

    return cl
}

// addMember adds a new node to the cluster.
// The new node will attempt to join existing nodes via the Peers list.
func (cl *testCacheCluster) addMember() *olricCache {
    cl.mtx.Lock()
    defer cl.mtx.Unlock()

    port := getNextPort()
    cfg := &OlricConfig{
        DMapName:     cl.dmapName,
        Embedded:     true,
        BindAddr:     fmt.Sprintf("127.0.0.1:%d", port),
        Environment:  "local",
        ReplicaCount: 2, // Enable replication for HA tests
    }

    // Add existing members as peers for discovery
    // Use the Olric bind address (memberlist port is handled internally)
    for _, m := range cl.members {
        if addr := m.MemberlistAddr(); addr != "" {
            cfg.Peers = append(cfg.Peers, addr)
        }
    }

    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    cache, err := newOlricCache(ctx, cfg)
    if err != nil {
        cl.t.Fatalf("failed to add cluster member: %v", err)
    }

    cl.members = append(cl.members, cache)

    // Give the cluster time to converge
    time.Sleep(500 * time.Millisecond)

    return cache
}

// shutdown closes all cluster members gracefully.
func (cl *testCacheCluster) shutdown() {
    cl.mtx.Lock()
    defer cl.mtx.Unlock()

    for _, m := range cl.members {
        if err := m.Close(); err != nil {
            cl.t.Logf("warning: failed to close cluster member: %v", err)
        }
    }
    cl.members = nil
}

// memberCount returns the number of nodes in the cluster.
func (cl *testCacheCluster) memberCount() int {
    cl.mtx.Lock()
    defer cl.mtx.Unlock()
    return len(cl.members)
}

// member returns the node at the given index.
func (cl *testCacheCluster) member(i int) *olricCache {
    cl.mtx.Lock()
    defer cl.mtx.Unlock()
    if i < 0 || i >= len(cl.members) {
        cl.t.Fatalf("member index %d out of range [0, %d)", i, len(cl.members))
    }
    return cl.members[i]
}

// waitForConvergence waits for all nodes to see the expected member count.
// Returns error if convergence doesn't happen within timeout.
func (cl *testCacheCluster) waitForConvergence(expectedMembers int, timeout time.Duration) error {
    cl.mtx.Lock()
    members := make([]*olricCache, len(cl.members))
    copy(members, cl.members)
    cl.mtx.Unlock()

    deadline := time.Now().Add(timeout)
    for time.Now().Before(deadline) {
        allConverged := true
        for _, m := range members {
            if count := m.ClusterMembers(); count != expectedMembers {
                allConverged = false
                break
            }
        }
        if allConverged {
            return nil
        }
        time.Sleep(100 * time.Millisecond)
    }
    return fmt.Errorf("cluster did not converge to %d members within %v", expectedMembers, timeout)
}
```
  </action>
  <verify>go build ./internal/cache/...</verify>
  <done>testutil.go provides testCacheCluster helper for managing multi-node test clusters</done>
</task>

<task type="auto">
  <name>Task 2: Create olric_cluster_test.go with multi-node tests</name>
  <files>internal/cache/olric_cluster_test.go</files>
  <action>
Create comprehensive multi-node cluster tests:

```go
//go:build integration
// +build integration

package cache

import (
    "bytes"
    "context"
    "testing"
    "time"
)

// TestOlricCluster_Formation tests that multiple nodes can form a cluster.
func TestOlricCluster_Formation(t *testing.T) {
    cluster := newTestCacheCluster(t)

    // Start first node
    node1 := cluster.addMember()
    t.Log("Node 1 started")

    // Should be single node initially
    if members := node1.ClusterMembers(); members != 1 {
        t.Errorf("Node 1 sees %d members, want 1", members)
    }

    // Add second node
    node2 := cluster.addMember()
    t.Log("Node 2 started")

    // Wait for cluster convergence
    err := cluster.waitForConvergence(2, 10*time.Second)
    if err != nil {
        t.Fatalf("Cluster failed to converge: %v", err)
    }

    // Both nodes should see 2 members
    if members := node1.ClusterMembers(); members != 2 {
        t.Errorf("Node 1 sees %d members after join, want 2", members)
    }
    if members := node2.ClusterMembers(); members != 2 {
        t.Errorf("Node 2 sees %d members after join, want 2", members)
    }

    t.Logf("Cluster formed successfully with 2 nodes")
}

// TestOlricCluster_DataReplication tests that data is replicated across nodes.
func TestOlricCluster_DataReplication(t *testing.T) {
    cluster := newTestCacheCluster(t)
    ctx := context.Background()

    // Create 2-node cluster
    node1 := cluster.addMember()
    node2 := cluster.addMember()

    // Wait for convergence
    err := cluster.waitForConvergence(2, 10*time.Second)
    if err != nil {
        t.Fatalf("Cluster failed to converge: %v", err)
    }

    // Write data to node 1
    testKey := "replicated-key"
    testValue := []byte("replicated-value")

    err = node1.Set(ctx, testKey, testValue)
    if err != nil {
        t.Fatalf("Set on node 1 failed: %v", err)
    }

    // Give replication time to complete
    time.Sleep(500 * time.Millisecond)

    // Read from node 2 - should see the replicated data
    got, err := node2.Get(ctx, testKey)
    if err != nil {
        t.Fatalf("Get on node 2 failed: %v", err)
    }

    if !bytes.Equal(got, testValue) {
        t.Errorf("Node 2 got %q, want %q", got, testValue)
    }

    t.Log("Data successfully replicated from node 1 to node 2")
}

// TestOlricCluster_NodeLeave tests that a node can leave gracefully.
func TestOlricCluster_NodeLeave(t *testing.T) {
    cluster := newTestCacheCluster(t)
    ctx := context.Background()

    // Create 2-node cluster
    node1 := cluster.addMember()
    node2 := cluster.addMember()

    // Wait for convergence
    err := cluster.waitForConvergence(2, 10*time.Second)
    if err != nil {
        t.Fatalf("Cluster failed to converge: %v", err)
    }

    // Write data while both nodes are up
    testKey := "survive-key"
    testValue := []byte("survive-value")

    err = node1.Set(ctx, testKey, testValue)
    if err != nil {
        t.Fatalf("Set on node 1 failed: %v", err)
    }

    // Give replication time
    time.Sleep(500 * time.Millisecond)

    // Close node 1 (graceful leave)
    t.Log("Shutting down node 1...")
    err = node1.Close()
    if err != nil {
        t.Errorf("Node 1 close failed: %v", err)
    }

    // Give cluster time to detect departure
    time.Sleep(1 * time.Second)

    // Node 2 should still be operational
    // Note: With ReplicaCount=2, node2 should have the data
    got, err := node2.Get(ctx, testKey)
    if err != nil {
        // This may fail if the data was owned by node1 and not replicated
        // With ReplicaCount=2, it should succeed
        t.Logf("Get after node leave returned error (may be expected with partition changes): %v", err)
    } else if !bytes.Equal(got, testValue) {
        t.Errorf("Node 2 got %q after node 1 left, want %q", got, testValue)
    } else {
        t.Log("Data survived node 1 departure (replica available on node 2)")
    }

    // Node 2 should see itself as single member now
    time.Sleep(500 * time.Millisecond)
    members := node2.ClusterMembers()
    t.Logf("Node 2 sees %d members after node 1 left", members)
}

// TestOlricCluster_DynamicJoin tests that new nodes can join an existing cluster.
func TestOlricCluster_DynamicJoin(t *testing.T) {
    cluster := newTestCacheCluster(t)
    ctx := context.Background()

    // Start with 2 nodes
    node1 := cluster.addMember()
    node2 := cluster.addMember()

    err := cluster.waitForConvergence(2, 10*time.Second)
    if err != nil {
        t.Fatalf("Initial cluster failed to converge: %v", err)
    }

    // Write some data
    err = node1.Set(ctx, "key1", []byte("value1"))
    if err != nil {
        t.Fatalf("Set on node 1 failed: %v", err)
    }

    time.Sleep(500 * time.Millisecond)

    // Add a third node dynamically
    t.Log("Adding node 3 to cluster...")
    node3 := cluster.addMember()

    // Wait for full convergence
    err = cluster.waitForConvergence(3, 15*time.Second)
    if err != nil {
        t.Fatalf("Cluster failed to converge after adding node 3: %v", err)
    }

    t.Logf("Node 3 joined, cluster has %d members", node3.ClusterMembers())

    // Node 3 should be able to read existing data
    got, err := node3.Get(ctx, "key1")
    if err != nil {
        t.Logf("Node 3 Get returned error (may be expected during partition rebalancing): %v", err)
    } else if !bytes.Equal(got, []byte("value1")) {
        t.Errorf("Node 3 got %q, want %q", got, "value1")
    } else {
        t.Log("Node 3 successfully read data written before it joined")
    }

    // Verify all nodes see 3 members
    for i, n := range []*olricCache{node1, node2, node3} {
        members := n.ClusterMembers()
        if members != 3 {
            t.Errorf("Node %d sees %d members, want 3", i+1, members)
        }
    }
}

// TestOlricCluster_ThreeNode tests a 3-node cluster with full replication.
func TestOlricCluster_ThreeNode(t *testing.T) {
    cluster := newTestCacheCluster(t)
    ctx := context.Background()

    // Create 3-node cluster
    node1 := cluster.addMember()
    node2 := cluster.addMember()
    node3 := cluster.addMember()

    err := cluster.waitForConvergence(3, 15*time.Second)
    if err != nil {
        t.Fatalf("Cluster failed to converge: %v", err)
    }

    t.Log("3-node cluster formed successfully")

    // Write data to each node
    testData := map[string][]byte{
        "from-node1": []byte("value-from-1"),
        "from-node2": []byte("value-from-2"),
        "from-node3": []byte("value-from-3"),
    }

    nodes := []*olricCache{node1, node2, node3}
    i := 0
    for key, value := range testData {
        if err := nodes[i].Set(ctx, key, value); err != nil {
            t.Fatalf("Set %q on node %d failed: %v", key, i+1, err)
        }
        i++
    }

    // Give replication time
    time.Sleep(1 * time.Second)

    // Each node should be able to read all data
    for nodeIdx, node := range nodes {
        for key, expectedValue := range testData {
            got, err := node.Get(ctx, key)
            if err != nil {
                t.Errorf("Node %d: Get %q failed: %v", nodeIdx+1, key, err)
                continue
            }
            if !bytes.Equal(got, expectedValue) {
                t.Errorf("Node %d: Get %q = %q, want %q", nodeIdx+1, key, got, expectedValue)
            }
        }
    }

    t.Log("All nodes can read all data - cluster is fully operational")
}
```

Note: These tests use the `integration` build tag. Run with: `go test -tags=integration ./internal/cache/...`
  </action>
  <verify>go build -tags=integration ./internal/cache/...</verify>
  <done>olric_cluster_test.go contains comprehensive multi-node integration tests</done>
</task>

<task type="auto">
  <name>Task 3: Add Taskfile target for cluster tests</name>
  <files>Taskfile.yml</files>
  <action>
Add a task to run the cluster integration tests. Read the existing Taskfile.yml first, then add:

```yaml
  test-cluster:
    desc: Run cache cluster integration tests
    cmds:
      - go test -tags=integration -v -timeout=120s ./internal/cache/... -run "TestOlricCluster"
```

Place this near the other test-* tasks in the file.
  </action>
  <verify>task --list | grep test-cluster</verify>
  <done>Taskfile has test-cluster target for running cluster integration tests</done>
</task>

</tasks>

<verification>
```bash
# Build succeeds with integration tag
go build -tags=integration ./internal/cache/...

# Regular tests still pass (cluster tests skipped without tag)
go test -v -short ./internal/cache/...

# Cluster tests run with integration tag (may take 30-60s)
go test -tags=integration -v -timeout=120s ./internal/cache/... -run "TestOlricCluster"

# Verify test files exist
ls internal/cache/testutil.go internal/cache/olric_cluster_test.go
```
</verification>

<success_criteria>
- testutil.go provides testCacheCluster helper with addMember, shutdown, waitForConvergence
- olric_cluster_test.go contains tests for:
  - Cluster formation (2 nodes discover each other)
  - Data replication (write to node1, read from node2)
  - Node leave (node1 leaves, node2 continues operating)
  - Dynamic join (node3 joins existing cluster)
  - Three-node cluster (full replication test)
- Tests use integration build tag (not run in regular test suite)
- Taskfile has test-cluster target
- All existing tests continue to pass
- Cluster tests pass when run with integration tag
</success_criteria>

<output>
After completion, create `.planning/phases/01.1-embedded-ha-cache-clustering/01.1-04-SUMMARY.md`
</output>
