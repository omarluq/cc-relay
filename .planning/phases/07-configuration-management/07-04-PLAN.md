---
phase: 07-configuration-management
plan: 04
type: execute
wave: 3
depends_on: ["07-02", "07-03"]
files_modified:
  - cmd/cc-relay/di/providers.go
  - cmd/cc-relay/di/providers_test.go
  - cmd/cc-relay/serve.go
  - cmd/cc-relay/serve_test.go
autonomous: true

must_haves:
  truths:
    - "Config changes are detected and reloaded without restart"
    - "In-flight requests continue with old config during reload"
    - "New requests use new config after reload"
    - "Watcher starts on server startup and stops on shutdown"
  artifacts:
    - path: "cmd/cc-relay/di/providers.go"
      provides: "ConfigService with atomic swap and watcher"
      contains: "atomic.Pointer"
    - path: "cmd/cc-relay/serve.go"
      provides: "Watcher lifecycle integration"
      contains: "StartWatching"
  key_links:
    - from: "cmd/cc-relay/di/providers.go"
      to: "internal/config/watcher.go"
      via: "Watcher creation and callback registration"
      pattern: "config\\.NewWatcher"
    - from: "cmd/cc-relay/di/providers.go"
      to: "sync/atomic"
      via: "atomic.Pointer for lock-free reads"
      pattern: "atomic\\.Pointer\\[config\\.Config\\]"
---

<objective>
Integrate config watcher with DI container for hot-reload support.

Purpose: Complete the hot-reload feature (CONF-05) by wiring the config watcher into the DI container. The ConfigService will use atomic.Pointer for lock-free config reads, enabling in-flight requests to continue uninterrupted while new requests use the reloaded config.

Output:
- ConfigService updated with atomic.Pointer[Config] for thread-safe access
- Watcher integrated into ConfigService with callback to swap config
- serve.go starts watcher on startup and includes in graceful shutdown
- Tests verify hot-reload behavior
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-configuration-management/07-RESEARCH.md
@cmd/cc-relay/di/providers.go
@cmd/cc-relay/serve.go
@internal/config/watcher.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update ConfigService with atomic pointer and watcher</name>
  <files>cmd/cc-relay/di/providers.go, cmd/cc-relay/di/providers_test.go</files>
  <action>
Update ConfigService to use atomic.Pointer for lock-free config reads and integrate the watcher.

**Update cmd/cc-relay/di/providers.go:**

Add new imports at top:
```go
import (
    // ... existing imports
    "sync/atomic"
    // ...
)
```

**Replace the ConfigService struct:**

```go
// ConfigService wraps the loaded configuration with hot-reload support.
// Uses atomic.Pointer for lock-free reads during request handling.
type ConfigService struct {
    current atomic.Pointer[config.Config]
    path    string
    watcher *config.Watcher
    logger  *zerolog.Logger
}

// Get returns the current configuration (lock-free read).
// This is safe to call from request handlers during hot-reload.
func (s *ConfigService) Get() *config.Config {
    return s.current.Load()
}

// StartWatching begins watching for config file changes.
// Call this after the server is ready to handle requests.
func (s *ConfigService) StartWatching(ctx context.Context) {
    if s.watcher == nil {
        return
    }
    s.watcher.OnReload(func(newCfg *config.Config) error {
        s.current.Store(newCfg)
        return nil
    })
    s.watcher.Start(ctx)
    s.logger.Info().Str("path", s.path).Msg("config hot-reload watching started")
}

// Shutdown stops the watcher and releases resources.
// Implements do.Shutdowner interface for graceful shutdown.
func (s *ConfigService) Shutdown() error {
    if s.watcher != nil {
        return s.watcher.Close()
    }
    return nil
}
```

**Update the NewConfig provider function:**

```go
// NewConfig loads configuration from the path stored in the DI container.
// Creates a ConfigService with hot-reload watcher support.
func NewConfig(i do.Injector) (*ConfigService, error) {
    path := do.MustInvokeNamed[string](i, ConfigPathKey)

    cfg, err := config.Load(path)
    if err != nil {
        return nil, fmt.Errorf("failed to load config from %s: %w", path, err)
    }

    // Get logger for watcher (may not exist yet, create nop logger)
    logger := zerolog.Nop()

    // Create watcher for hot-reload
    watcher, err := config.NewWatcher(path, &logger)
    if err != nil {
        // Log warning but don't fail - hot-reload is nice-to-have
        logger.Warn().Err(err).Msg("config hot-reload disabled (watcher creation failed)")
        watcher = nil
    }

    svc := &ConfigService{
        path:    path,
        watcher: watcher,
        logger:  &logger,
    }
    svc.current.Store(cfg)

    return svc, nil
}
```

**Update NewLogger to receive ConfigService and update watcher logger:**

After NewLogger creates the logger, update the ConfigService watcher's logger:
```go
// NewLogger creates a zerolog logger based on configuration.
func NewLogger(i do.Injector) (*LoggerService, error) {
    cfgSvc := do.MustInvoke[*ConfigService](i)
    cfg := cfgSvc.Get()

    // ... existing logger creation logic ...

    // Update ConfigService with real logger for watcher
    cfgSvc.logger = &logger

    return &LoggerService{Logger: &logger}, nil
}
```

**Add test in cmd/cc-relay/di/providers_test.go:**

```go
func TestConfigService_HotReload(t *testing.T) {
    tmpDir := t.TempDir()
    configPath := filepath.Join(tmpDir, "config.yaml")

    // Write initial config
    initialConfig := `
server:
  listen: ":8787"
providers:
  - name: test
    type: anthropic
    enabled: true
    keys:
      - key: initial-key
cache:
  mode: single
  ristretto:
    max_cost: 104857600
    num_counters: 1000000
`
    err := os.WriteFile(configPath, []byte(initialConfig), 0o644)
    require.NoError(t, err)

    // Create injector with config
    i := do.New()
    do.ProvideNamedValue(i, ConfigPathKey, configPath)
    do.Provide(i, NewConfig)

    cfgSvc, err := do.Invoke[*ConfigService](i)
    require.NoError(t, err)

    // Verify initial config
    cfg := cfgSvc.Get()
    assert.Equal(t, "initial-key", cfg.Providers[0].Keys[0].Key)

    // Start watching
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()
    cfgSvc.StartWatching(ctx)

    // Update config file
    time.Sleep(50 * time.Millisecond)
    updatedConfig := `
server:
  listen: ":8787"
providers:
  - name: test
    type: anthropic
    enabled: true
    keys:
      - key: updated-key
cache:
  mode: single
  ristretto:
    max_cost: 104857600
    num_counters: 1000000
`
    err = os.WriteFile(configPath, []byte(updatedConfig), 0o644)
    require.NoError(t, err)

    // Wait for reload
    time.Sleep(200 * time.Millisecond)

    // Verify config was reloaded
    cfg = cfgSvc.Get()
    assert.Equal(t, "updated-key", cfg.Providers[0].Keys[0].Key)

    // Shutdown
    err = cfgSvc.Shutdown()
    assert.NoError(t, err)
}

func TestConfigService_Get_LockFree(t *testing.T) {
    tmpDir := t.TempDir()
    configPath := filepath.Join(tmpDir, "config.yaml")

    content := `
server:
  listen: ":8787"
providers:
  - name: test
    type: anthropic
    enabled: true
    keys:
      - key: test-key
cache:
  mode: single
  ristretto:
    max_cost: 104857600
    num_counters: 1000000
`
    err := os.WriteFile(configPath, []byte(content), 0o644)
    require.NoError(t, err)

    i := do.New()
    do.ProvideNamedValue(i, ConfigPathKey, configPath)
    do.Provide(i, NewConfig)

    cfgSvc, err := do.Invoke[*ConfigService](i)
    require.NoError(t, err)

    // Concurrent reads should not block
    done := make(chan bool)
    for j := 0; j < 100; j++ {
        go func() {
            cfg := cfgSvc.Get()
            assert.NotNil(t, cfg)
            done <- true
        }()
    }

    for j := 0; j < 100; j++ {
        <-done
    }
}
```

**IMPORTANT:**
- Keep backward compatibility - existing code calling cfgSvc.Config should work
- Add Config field as alias: `func (s *ConfigService) Config() *config.Config { return s.Get() }`
- Handle case where watcher creation fails (e.g., permission issues)
  </action>
  <verify>
`go build ./cmd/cc-relay/...` succeeds
`go test ./cmd/cc-relay/di/... -v -run TestConfigService` passes
  </verify>
  <done>
ConfigService uses atomic.Pointer, watcher integrated, tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate watcher lifecycle into serve.go</name>
  <files>cmd/cc-relay/serve.go, cmd/cc-relay/serve_test.go</files>
  <action>
Start the config watcher after server initialization and include in graceful shutdown.

**Update cmd/cc-relay/serve.go:**

In the runWithGracefulShutdown function (or equivalent), after DI container is created and before server starts:

```go
// Start config hot-reload watcher
cfgSvc := do.MustInvoke[*ConfigService](container.Injector())
cfgSvc.StartWatching(ctx)
```

The ConfigService.Shutdown() is already called automatically by DI container shutdown since it implements do.Shutdowner.

If serve.go uses a different pattern, find the appropriate place to call:
1. `cfgSvc.StartWatching(ctx)` after server is ready
2. Ensure container.Shutdown() is called on graceful shutdown (this calls ConfigService.Shutdown())

**Add test in cmd/cc-relay/serve_test.go:**

```go
func TestServe_ConfigWatcherLifecycle(t *testing.T) {
    // This test verifies the watcher starts and stops with the server
    // The actual hot-reload behavior is tested in providers_test.go

    tmpDir := t.TempDir()
    configPath := filepath.Join(tmpDir, "config.yaml")

    content := `
server:
  listen: ":0"
providers:
  - name: test
    type: anthropic
    enabled: true
    keys:
      - key: test-key
cache:
  mode: single
  ristretto:
    max_cost: 104857600
    num_counters: 1000000
`
    err := os.WriteFile(configPath, []byte(content), 0o644)
    require.NoError(t, err)

    // Create container
    container, err := di.NewContainer(configPath)
    require.NoError(t, err)

    // Get config service
    cfgSvc := do.MustInvoke[*ConfigService](container.Injector())

    // Start watching
    ctx, cancel := context.WithCancel(context.Background())
    cfgSvc.StartWatching(ctx)

    // Verify watcher is running (can detect file changes)
    time.Sleep(50 * time.Millisecond)

    // Shutdown
    cancel()
    err = container.Shutdown()
    assert.NoError(t, err)
}
```

**Logging:**
Add log messages for config reload events:
- "config hot-reload watching started" (Info, on StartWatching)
- "configuration reloaded successfully" (Info, on successful reload - already in watcher.go)
- "failed to reload config, keeping current" (Error, on invalid config - already in watcher.go)
  </action>
  <verify>
`go build ./cmd/cc-relay/...` succeeds
`go test ./cmd/cc-relay/... -v -run TestServe` passes
`task test` passes full test suite
  </verify>
  <done>
Watcher lifecycle integrated with server startup/shutdown
  </done>
</task>

</tasks>

<verification>
Run all checks to verify plan completion:

```bash
# Build succeeds
go build ./cmd/cc-relay/...

# Config service tests pass
go test ./cmd/cc-relay/di/... -v -run "ConfigService"

# Serve tests pass
go test ./cmd/cc-relay/... -v -run "Serve"

# Full test suite passes
task test

# Linters pass
task lint
```
</verification>

<success_criteria>
1. ConfigService.Get() returns config via atomic.Pointer (lock-free)
2. ConfigService.StartWatching() registers callback and starts watcher
3. Config changes trigger reload and atomic swap
4. In-flight requests see old config, new requests see new config
5. Server shutdown cleanly stops watcher
6. All tests pass, linters pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-configuration-management/07-04-SUMMARY.md`
</output>
