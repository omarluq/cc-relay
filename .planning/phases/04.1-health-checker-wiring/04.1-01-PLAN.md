---
phase: 04.1-health-checker-wiring
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - cmd/cc-relay/di/providers.go
  - cmd/cc-relay/serve.go
  - cmd/cc-relay/di/providers_test.go
autonomous: true

must_haves:
  truths:
    - "Checker.Start() is called during application startup"
    - "All enabled providers are registered with Checker via RegisterProvider()"
    - "Periodic health checks run for providers with OPEN circuits"
    - "Graceful shutdown stops the Checker before container cleanup"
  artifacts:
    - path: "cmd/cc-relay/di/providers.go"
      provides: "Provider registration in NewChecker"
      contains: "checker.RegisterProvider"
    - path: "cmd/cc-relay/serve.go"
      provides: "Checker startup call"
      contains: "checkerSvc.Checker.Start()"
    - path: "cmd/cc-relay/di/providers_test.go"
      provides: "Integration test for Checker lifecycle"
      contains: "TestChecker_StartsAndStopsWithContainer"
  key_links:
    - from: "cmd/cc-relay/serve.go"
      to: "CheckerService.Checker.Start()"
      via: "DI invoke then method call"
      pattern: "MustInvoke.*CheckerService.*Start"
    - from: "cmd/cc-relay/di/providers.go NewChecker"
      to: "health.RegisterProvider"
      via: "loop over enabled providers"
      pattern: "checker.RegisterProvider"
    - from: "cmd/cc-relay/di/providers.go NewChecker"
      to: "health.NewProviderHealthCheck"
      via: "factory function call"
      pattern: "NewProviderHealthCheck.*Name.*BaseURL"
---

<objective>
Wire Checker.Start() and RegisterProvider() to make periodic health checks operational.

Purpose: Close the integration gap from Phase 4 - all health components exist but the Checker is never started and no providers are registered. This plan wires the existing components to complete the health checking feature.

Output:
- Modified NewChecker in providers.go that registers health checks for all enabled providers
- Modified runServe in serve.go that starts the Checker after DI initialization
- Integration test verifying Checker lifecycle works end-to-end
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-health-checker-wiring/04.1-RESEARCH.md

# Key source files
@cmd/cc-relay/di/providers.go
@cmd/cc-relay/serve.go
@internal/health/checker.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Register providers in NewChecker</name>
  <files>cmd/cc-relay/di/providers.go</files>
  <action>
Modify the `NewChecker` function to register health checks for all enabled providers after creating the Checker.

Add a loop after `health.NewChecker()` call:

```go
// Register health checks for all enabled providers
for idx := range cfgSvc.Config.Providers {
    pc := &cfgSvc.Config.Providers[idx]
    if !pc.Enabled {
        continue
    }
    // NewProviderHealthCheck handles empty BaseURL (returns NoOpHealthCheck)
    healthCheck := health.NewProviderHealthCheck(pc.Name, pc.BaseURL, nil)
    checker.RegisterProvider(healthCheck)
    loggerSvc.Logger.Debug().
        Str("provider", pc.Name).
        Str("base_url", pc.BaseURL).
        Msg("registered health check")
}
```

This pattern matches the existing provider iteration in `NewProxyHandler` (lines 309-335).

**Why this approach:**
- Registration in DI factory keeps config access centralized (not in serve.go)
- Same pattern as NewProxyHandler ensures consistency
- Debug logging helps troubleshoot provider registration issues
- `NewProviderHealthCheck` handles empty BaseURL by returning NoOpHealthCheck (already implemented)
  </action>
  <verify>
Run: `go build ./cmd/cc-relay && go test ./cmd/cc-relay/di/... -v -run TestNewChecker`
Verify: No compilation errors, existing tests pass
  </verify>
  <done>NewChecker registers health checks for all enabled providers during DI initialization</done>
</task>

<task type="auto">
  <name>Task 2: Start Checker in serve.go</name>
  <files>cmd/cc-relay/serve.go</files>
  <action>
Add Checker startup in `runServe` function, after getting ServerService and before `runWithGracefulShutdown`.

Insert after line 93 (after serverSvc creation, before return):

```go
// Start health checker (after all DI services initialized)
checkerSvc := di.MustInvoke[*di.CheckerService](container)
checkerSvc.Checker.Start()
```

**Why this location:**
- After ServerService ensures all DI dependencies initialized (Checker depends on Tracker, Config, Logger)
- Before runWithGracefulShutdown ensures Checker runs during server lifetime
- MustInvoke is safe here because Checker is already invoked as a dependency of ServerService

**Why not in NewChecker:**
- DI initialization order isn't guaranteed to have all services ready
- Starting in serve.go gives explicit control over startup sequence
- Matches pattern: DI creates, serve.go starts

**Shutdown handled automatically:**
- CheckerService already implements `do.Shutdowner` interface (Shutdown calls Checker.Stop)
- container.ShutdownWithContext in runWithGracefulShutdown calls all Shutdowner implementations
- DI shutdown is LIFO, so Checker stops after Server (correct order)
  </action>
  <verify>
Run: `go build ./cmd/cc-relay`
Verify: No compilation errors
Manual: Start server with a config that has health.health_check enabled, check logs for "health checker started"
  </verify>
  <done>Checker.Start() called during application startup, after all DI services initialized</done>
</task>

<task type="auto">
  <name>Task 3: Add integration test for Checker lifecycle</name>
  <files>cmd/cc-relay/di/providers_test.go</files>
  <action>
Add integration test that verifies the complete Checker lifecycle:

```go
func TestChecker_StartsAndStopsWithContainer(t *testing.T) {
    // Create minimal config with health check enabled
    cfg := &config.Config{
        Providers: []config.ProviderConfig{
            {
                Name:    "test-provider",
                Type:    "anthropic",
                Enabled: true,
                BaseURL: "http://localhost:9999", // Fake URL - we just test lifecycle
                Keys: []config.KeyConfig{
                    {Key: "test-key"},
                },
            },
        },
        Health: health.Config{
            HealthCheck: health.CheckConfig{
                Enabled:    ptrBool(true),
                IntervalMS: 100, // Fast interval for testing
            },
            CircuitBreaker: health.CircuitBreakerConfig{
                FailureThreshold: 5,
                RecoveryTimeoutMS: 1000,
            },
        },
        Server: config.ServerConfig{
            Listen: "localhost:0",
        },
        Logging: config.LoggingConfig{
            Level: "debug",
        },
    }

    // Create test container with pre-configured services
    container := do.New()
    nopLogger := zerolog.Nop()
    do.ProvideValue(container, &ConfigService{Config: cfg})
    do.ProvideValue(container, &LoggerService{Logger: &nopLogger})
    do.Provide(container, NewHealthTracker)
    do.Provide(container, NewChecker)

    // Get checker and verify provider was registered (via Debug log or internal state)
    checkerSvc := do.MustInvoke[*CheckerService](container)
    require.NotNil(t, checkerSvc.Checker, "Checker should be created")

    // Start the checker
    checkerSvc.Checker.Start()

    // Give it time to run at least one check cycle
    time.Sleep(150 * time.Millisecond)

    // Shutdown via container (tests graceful shutdown path)
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    err := container.ShutdownWithContext(ctx)
    require.NoError(t, err, "Container shutdown should succeed")

    // If we got here without deadlock or panic, the lifecycle works
}

// Helper for pointer bool (if not already exists)
func ptrBool(b bool) *bool {
    return &b
}
```

**Test verifies:**
1. NewChecker creates Checker successfully
2. Provider registration happens (via config)
3. Start() runs without error
4. Container shutdown calls Checker.Stop() via Shutdowner interface
5. No goroutine leaks or deadlocks

**Note:** This test uses a fake URL because we're testing lifecycle, not actual health checking.
The health check will fail but that's fine - we just need to verify the goroutine starts and stops correctly.
  </action>
  <verify>
Run: `go test ./cmd/cc-relay/di/... -v -run TestChecker_StartsAndStopsWithContainer`
Verify: Test passes, no goroutine leaks detected
  </verify>
  <done>Integration test verifies Checker lifecycle works end-to-end</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Build check:**
   ```bash
   go build ./cmd/cc-relay
   ```

2. **Unit tests:**
   ```bash
   go test ./cmd/cc-relay/di/... -v
   ```

3. **Full test suite:**
   ```bash
   task test
   ```

4. **Lint check:**
   ```bash
   task lint
   ```

5. **Manual verification (optional):**
   Start server with debug logging and health check enabled:
   ```bash
   ./bin/cc-relay serve --debug --config example.yaml
   ```
   Look for log messages:
   - "registered health check" (from NewChecker)
   - "health checker started" (from Checker.Start)
</verification>

<success_criteria>
- [ ] NewChecker registers health checks for all enabled providers
- [ ] Checker.Start() called in serve.go after DI initialization
- [ ] Integration test passes for Checker lifecycle
- [ ] All existing tests pass
- [ ] Linter passes
- [ ] Server logs show "health checker started" on startup with health checks enabled
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-health-checker-wiring/04.1-01-SUMMARY.md`
</output>
